# Orchestrator Script Refinement Plan

This document outlines the necessary refinements to the `phoenix_orchestrator.sh` script to resolve deployment failures related to shared volume permissions. The core issue is a race condition where the script attempts to read UID mappings (`idmap`) for unprivileged containers before those mappings have been generated.

## 1. Analysis of the Root Cause

The deployment failures stem from two primary logical flaws in the script:

1.  **Incorrect Timing of `idmap` Generation:** The script attempts to read the `idmap` from a container's configuration file *before* the container has been set as unprivileged. The `idmap` is only generated by Proxmox *after* the `--unprivileged 1` flag has been successfully applied.
2.  **Over-reaching Scope in Volume Application:** The `apply_shared_volumes` function iterates through all containers associated with a shared volume, even those not part of the current orchestration run. This leads to errors when it tries to get the mapped UID for containers that do not yet exist.

## 2. Proposed Logical Changes

To resolve these issues, the following changes will be made to the script's logic:

1.  **Introduce `ensure_container_defined` function:** A new function, `ensure_container_defined`, will be created to handle the initial creation or cloning of a container. This function will be responsible for setting the `--unprivileged 1` flag immediately after the container is defined, ensuring the `idmap` is available for subsequent steps.
2.  **Refine `get_container_mapped_root_uid` function:** This function will be made more robust with explicit checks for the existence of the container's configuration file and the `idmap` line within it. It will return a non-zero status and log a clear warning if either is not found, preventing the script from failing catastrophically.
3.  **Refine `apply_shared_volumes` function:** The logic for setting `container_root` ownership will be modified to only act on the container specified in the current orchestration run (`$CTID`). This prevents the script from attempting to access containers that have not yet been created.

## 3. Corrected Bash Code

The following sections provide the complete, final, and corrected bash code for the affected functions.

### `ensure_container_defined`

This new function encapsulates the container creation/cloning logic and ensures the `unprivileged` flag is set immediately.

```bash
# =====================================================================================
# Function: ensure_container_defined
# Description: Ensures a container is defined, either by creating it from a template
#              or cloning it from another container. It then immediately sets the
#              unprivileged flag to ensure the idmap is generated.
# Arguments:
#   None (uses global CTID).
# Returns:
#   None.
# =====================================================================================
ensure_container_defined() {
    log_info "Ensuring container $CTID is defined..."
    if ! pct status "$CTID" > /dev/null 2>&1; then
        log_info "Container $CTID does not exist. Proceeding with creation..."
        local clone_from_ctid
        clone_from_ctid=$(jq_get_value "$CTID" ".clone_from_ctid" || echo "")

        if [ -n "$clone_from_ctid" ]; then
            clone_container
        else
            create_container_from_template
        fi

        # Set unprivileged flag immediately after creation to generate idmap
        local unprivileged_bool
        unprivileged_bool=$(jq_get_value "$CTID" ".unprivileged")
        if [ "$unprivileged_bool" == "true" ]; then
            log_info "Setting container $CTID as unprivileged..."
            run_pct_command set "$CTID" --unprivileged 1 || log_fatal "Failed to set container as unprivileged."
        fi
    else
        log_info "Container $CTID already exists. Skipping creation."
    fi
}
```

### `get_container_mapped_root_uid`

This function is updated to be more robust and provide clearer error messaging.

```bash
# =====================================================================================
# Function: get_container_mapped_root_uid
# Description: Retrieves the mapped root UID for an unprivileged container from its
#              configuration file.
# Arguments:
#   $1 - The container ID (CTID).
# Returns:
#   The mapped root UID on success, or an empty string on failure.
# =====================================================================================
get_container_mapped_root_uid() {
    local container_id="$1"
    local conf_file="/etc/pve/lxc/${container_id}.conf"

    if [ ! -f "$conf_file" ]; then
        log_warn "Container config file not found: $conf_file. Cannot get mapped UID."
        return 1
    fi

    local idmap_line
    idmap_line=$(grep "^lxc.idmap:" "$conf_file")

    if [ -z "$idmap_line" ]; then
        log_warn "No idmap found in $conf_file. The container might not be unprivileged or the idmap has not been generated yet."
        return 1
    fi

    # Example lxc.idmap line: lxc.idmap: u 0 100000 65536
    local mapped_uid
    mapped_uid=$(echo "$idmap_line" | awk '{print $3}')
    echo "$mapped_uid"
}
```

### `apply_shared_volumes`

This function is refined to only operate on the container currently being orchestrated.

```bash
# =====================================================================================
# Function: apply_shared_volumes
# Description: Applies shared volume configurations to the specified container.
# Arguments:
#   None (uses global CTID and VM_CONFIG_FILE).
# Returns:
#   None.
# =====================================================================================
apply_shared_volumes() {
    log_info "Applying shared volumes for CTID: $CTID..."
    local shared_volumes
    shared_volumes=$(jq -c '.shared_volumes // {}' "$VM_CONFIG_FILE")
    local containers_to_restart=()

    for volume_name in $(echo "$shared_volumes" | jq -r 'keys[]'); do
        local volume_config
        volume_config=$(echo "$shared_volumes" | jq -r --arg name "$volume_name" '.[$name]')
        local host_path
        host_path=$(echo "$volume_config" | jq -r '.host_path')
        local owner
        owner=$(echo "$volume_config" | jq -r '.owner // ""')
        local mounts
        mounts=$(echo "$volume_config" | jq -c '.mounts')

        # Check if the current container has a mount defined for this volume
        if ! echo "$mounts" | jq -e --arg ctid "$CTID" '.[$ctid]' > /dev/null; then
            continue # Skip this volume if the current container is not in its mount list
        fi

        if [ ! -d "$host_path" ]; then
            log_info "Creating shared directory on host: $host_path"
            if ! mkdir -p "$host_path"; then
                log_fatal "Failed to create host directory: $host_path"
            fi
        fi

        if [ "$owner" == "container_root" ]; then
            log_info "Setting ownership to container_root for: $host_path"
            local mapped_uid
            mapped_uid=$(get_container_mapped_root_uid "$CTID")
            if [ -n "$mapped_uid" ]; then
                log_info "chown-ing to mapped root UID $mapped_uid for container $CTID"
                if ! chown -R "$mapped_uid:$mapped_uid" "$host_path"; then
                    log_fatal "Failed to set ownership for container_root on $host_path"
                fi
            else
                log_warn "Could not determine mapped root UID for container $CTID. Defaulting to nobody:nogroup."
                if ! chown -R nobody:nogroup "$host_path"; then
                    log_fatal "Failed to set ownership to nobody:nogroup on $host_path"
                fi
            fi
        elif [ -n "$owner" ]; then
            log_info "Setting ownership to user '$owner' for: $host_path"
            if ! chown -R "$owner:$owner" "$host_path"; then
                log_fatal "Failed to set ownership to user '$owner' on $host_path"
            fi
        else
            log_info "Setting ownership to nobody:nogroup for: $host_path"
            if ! chown -R nobody:nogroup "$host_path"; then
                log_fatal "Failed to set ownership to nobody:nogroup on $host_path"
            fi
        fi

        # Apply permissions based on volume name
        log_info "Applying permissions for shared volume: $volume_name"
        case "$volume_name" in
            "ssl_certs")
                ensure_shared_ssl_certs_exist "$host_path"
                find "$host_path" -type d -exec chmod 755 {} \; || log_fatal "Failed to set directory permissions for ssl_certs"
                find "$host_path" -type f -exec chmod 644 {} \; || log_fatal "Failed to set file permissions for ssl_certs"
                ;;
            "portainer_data")
                find "$host_path" -type d -exec chmod 770 {} \; || log_fatal "Failed to set directory permissions for portainer_data"
                find "$host_path" -type f -exec chmod 660 {} \; || log_fatal "Failed to set file permissions for portainer_data"
                ;;
            "nginx_sites")
                find "$host_path" -type d -exec chmod 755 {} \; || log_fatal "Failed to set directory permissions for nginx_sites"
                find "$host_path" -type f -exec chmod 644 {} \; || log_fatal "Failed to set file permissions for nginx_sites"
                ;;
            *)
                find "$host_path" -type d -exec chmod 775 {} \; || log_fatal "Failed to set default directory permissions"
                find "$host_path" -type f -exec chmod 664 {} \; || log_fatal "Failed to set default file permissions"
                ;;
        esac

        local mount_point
        mount_point=$(echo "$mounts" | jq -r --arg ctid "$CTID" '.[$ctid]')
        
        if ! pct config "$CTID" | grep -q "mp[0-9]*:.*,mp=${mount_point}"; then
            log_info "Creating mount point for CTID $CTID: $host_path -> $mount_point"
            local mp_num=0
            while pct config "$CTID" | grep -q "mp${mp_num}:"; do
                mp_num=$((mp_num + 1))
            done
            run_pct_command set "$CTID" --mp${mp_num} "${host_path},mp=${mount_point}"
            
            if [[ ! " ${containers_to_restart[*]} " =~ " ${CTID} " ]]; then
                containers_to_restart+=("$CTID")
            fi
        else
            log_info "Mount point already exists for CTID $CTID: $mount_point"
        fi
    done

    if [ ${#containers_to_restart[@]} -gt 0 ]; then
        log_info "Restarting containers to apply mount point changes: ${containers_to_restart[*]}"
        for ctid_to_restart in "${containers_to_restart[@]}"; do
            if pct status "$ctid_to_restart" | grep -q "running"; then
                log_info "Stopping container $ctid_to_restart..."
                run_pct_command stop "$ctid_to_restart"
                log_info "Starting container $ctid_to_restart..."
                start_container "$ctid_to_restart"
            else
                log_info "Container $ctid_to_restart is not running. No restart needed."
            fi
        done
    fi

    log_info "Shared volumes applied successfully for CTID: $CTID."
}